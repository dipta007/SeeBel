{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from yacs.config import CfgNode as CN\n",
    "import os\n",
    "from PIL import Image \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from cityscapesscripts.helpers.labels import trainId2label as t2l\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "import shutil\n",
    "import time\n",
    "import tqdm\n",
    "from csv import reader\n",
    "from functools import reduce\n",
    "import torchvision\n",
    "import torch._utils\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import math\n",
    "from torchvision import models\n",
    "import progressbar\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "import gc\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import logging\n",
    "import functools\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/home/sourajit/DeepLearningProjects/data/cityscapes'\n",
    "VAL_PRED_LOCATION = '/home/sourajit/DeepLearningProjects/Research_2.0/Semantic_Segmentation/Cityscapes_Experiments/Trial_005/'\n",
    "TEST_PRED_LOCATION = '/home/sourajit/DeepLearningProjects/Research_2.0/Semantic_Segmentation/Cityscapes_Experiments/Trial_005/'\n",
    "MODEL_PATH = '/home/sourajit/DeepLearningProjects/Research_2.0/Semantic_Segmentation/Cityscapes_Experiments/Trial_005/model.pth'\n",
    "TRAIN_LOSS_DIR = '/home/sourajit/DeepLearningProjects/Research_2.0/Semantic_Segmentation/Cityscapes_Experiments/Trial_005/Training_Loss_log.csv'\n",
    "TRAIN_meanIOU_DIR = '/home/sourajit/DeepLearningProjects/Research_2.0/Semantic_Segmentation/Cityscapes_Experiments/Trial_005/Training_meanIOU_log.csv'\n",
    "TRAIN_LR_DIR = '/home/sourajit/DeepLearningProjects/Research_2.0/Semantic_Segmentation/Cityscapes_Experiments/Trial_005/Training_LR_log.csv'\n",
    "TRAIN_MODEL_UPDATE_DIR = '/home/sourajit/DeepLearningProjects/Research_2.0/Semantic_Segmentation/Cityscapes_Experiments/Trial_005/Training_model_update_log.csv'\n",
    "Global_Epoch_Counter_DIR = '/home/sourajit/DeepLearningProjects/Research_2.0/Semantic_Segmentation/Cityscapes_Experiments/Trial_005/Global_Epoch_Counter.csv'\n",
    "TRAIN_LOSS = []\n",
    "TRAIN_mIOU = []\n",
    "TRAIN_LR = []\n",
    "TRAIN_MODEL_UPDATE = []\n",
    "TRAIN_MODEL_UPDATE_DIR\n",
    "BEST_mIOU = 0\n",
    "IMG_HEIGHT = 512  \n",
    "IMG_WIDTH = 1024\n",
    "ORIGINAL_IMG_HEIGHT = 1024  \n",
    "ORIGINAL_IMG_WIDTH = 2048\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 10\n",
    "GLOBAL_EPOCH_COUNTER = 0\n",
    "MAX_ITER = 3000000\n",
    "NUM_WORKERS = 8\n",
    "NUM_CLASSES = 19\n",
    "ignore_label = 255\n",
    "BatchNorm2d = nn.BatchNorm2d\n",
    "BN_MOMENTUM = 0.01\n",
    "ALIGN_CORNERS = True\n",
    "logger = logging.getLogger(__name__)\n",
    "device_rtx3090 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device_rtx3070 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchNorm2d = nn.BatchNorm2d\n",
    "BN_MOMENTUM = 0.01\n",
    "ALIGN_CORNERS = True\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = BatchNorm2d(planes * self.expansion,\n",
    "                               momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighResolutionModule(nn.Module):\n",
    "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
    "                 num_channels, fuse_method, multi_scale_output=True):\n",
    "        super(HighResolutionModule, self).__init__()\n",
    "        self._check_branches(\n",
    "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
    "\n",
    "        self.num_inchannels = num_inchannels\n",
    "        self.fuse_method = fuse_method\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        self.multi_scale_output = multi_scale_output\n",
    "\n",
    "        self.branches = self._make_branches(\n",
    "            num_branches, blocks, num_blocks, num_channels)\n",
    "        self.fuse_layers = self._make_fuse_layers()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def _check_branches(self, num_branches, blocks, num_blocks,\n",
    "                        num_inchannels, num_channels):\n",
    "        if num_branches != len(num_blocks):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n",
    "                num_branches, len(num_blocks))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_channels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n",
    "                num_branches, len(num_channels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_inchannels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n",
    "                num_branches, len(num_inchannels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
    "                         stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or \\\n",
    "           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.num_inchannels[branch_index],\n",
    "                          num_channels[branch_index] * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(num_channels[branch_index] * block.expansion,\n",
    "                            momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.num_inchannels[branch_index],\n",
    "                            num_channels[branch_index], stride, downsample))\n",
    "        self.num_inchannels[branch_index] = \\\n",
    "            num_channels[branch_index] * block.expansion\n",
    "        for i in range(1, num_blocks[branch_index]):\n",
    "            layers.append(block(self.num_inchannels[branch_index],\n",
    "                                num_channels[branch_index]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
    "        branches = []\n",
    "\n",
    "        for i in range(num_branches):\n",
    "            branches.append(\n",
    "                self._make_one_branch(i, block, num_blocks, num_channels))\n",
    "\n",
    "        return nn.ModuleList(branches)\n",
    "\n",
    "    def _make_fuse_layers(self):\n",
    "        if self.num_branches == 1:\n",
    "            return None\n",
    "\n",
    "        num_branches = self.num_branches\n",
    "        num_inchannels = self.num_inchannels\n",
    "        fuse_layers = []\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\n",
    "            fuse_layer = []\n",
    "            for j in range(num_branches):\n",
    "                if j > i:\n",
    "                    fuse_layer.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_inchannels[j],\n",
    "                                  num_inchannels[i],\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  0,\n",
    "                                  bias=False),\n",
    "                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n",
    "                elif j == i:\n",
    "                    fuse_layer.append(None)\n",
    "                else:\n",
    "                    conv3x3s = []\n",
    "                    for k in range(i-j):\n",
    "                        if k == i - j - 1:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                BatchNorm2d(num_outchannels_conv3x3, \n",
    "                                            momentum=BN_MOMENTUM)))\n",
    "                        else:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                BatchNorm2d(num_outchannels_conv3x3,\n",
    "                                            momentum=BN_MOMENTUM),\n",
    "                                nn.ReLU(inplace=True)))\n",
    "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
    "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
    "\n",
    "        return nn.ModuleList(fuse_layers)\n",
    "\n",
    "    def get_num_inchannels(self):\n",
    "        return self.num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_branches == 1:\n",
    "            return [self.branches[0](x[0])]\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            x[i] = self.branches[i](x[i])\n",
    "\n",
    "        x_fuse = []\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
    "            for j in range(1, self.num_branches):\n",
    "                if i == j:\n",
    "                    y = y + x[j]\n",
    "                elif j > i:\n",
    "                    width_output = x[i].shape[-1]\n",
    "                    height_output = x[i].shape[-2]\n",
    "                    y = y + F.interpolate(\n",
    "                        self.fuse_layers[i][j](x[j]),\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear')\n",
    "                else:\n",
    "                    y = y + self.fuse_layers[i][j](x[j])\n",
    "            x_fuse.append(self.relu(y))\n",
    "\n",
    "        return x_fuse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_dict = {\n",
    "    'BASIC': BasicBlock,\n",
    "    'BOTTLENECK': Bottleneck\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_RESOLUTION_NET = CN()\n",
    "HIGH_RESOLUTION_NET.PRETRAINED_LAYERS = ['*']\n",
    "HIGH_RESOLUTION_NET.STEM_INPLANES = 64\n",
    "HIGH_RESOLUTION_NET.FINAL_CONV_KERNEL = 1\n",
    "HIGH_RESOLUTION_NET.WITH_HEAD = True\n",
    "\n",
    "HIGH_RESOLUTION_NET.STAGE1 = CN()\n",
    "HIGH_RESOLUTION_NET.STAGE1.NUM_MODULES = 1\n",
    "HIGH_RESOLUTION_NET.STAGE1.NUM_BRANCHES = 1\n",
    "HIGH_RESOLUTION_NET.STAGE1.NUM_BLOCKS = [4]\n",
    "HIGH_RESOLUTION_NET.STAGE1.NUM_CHANNELS = [64]\n",
    "HIGH_RESOLUTION_NET.STAGE1.BLOCK = 'BOTTLENECK'\n",
    "HIGH_RESOLUTION_NET.STAGE1.FUSE_METHOD = 'SUM'\n",
    "\n",
    "HIGH_RESOLUTION_NET.STAGE2 = CN()\n",
    "HIGH_RESOLUTION_NET.STAGE2.NUM_MODULES = 1\n",
    "HIGH_RESOLUTION_NET.STAGE2.NUM_BRANCHES = 2\n",
    "HIGH_RESOLUTION_NET.STAGE2.NUM_BLOCKS = [4, 4]\n",
    "HIGH_RESOLUTION_NET.STAGE2.NUM_CHANNELS = [48, 96]\n",
    "HIGH_RESOLUTION_NET.STAGE2.BLOCK = 'BASIC'\n",
    "HIGH_RESOLUTION_NET.STAGE2.FUSE_METHOD = 'SUM'\n",
    "\n",
    "HIGH_RESOLUTION_NET.STAGE3 = CN()\n",
    "HIGH_RESOLUTION_NET.STAGE3.NUM_MODULES = 4\n",
    "HIGH_RESOLUTION_NET.STAGE3.NUM_BRANCHES = 3\n",
    "HIGH_RESOLUTION_NET.STAGE3.NUM_BLOCKS = [4, 4, 4]\n",
    "HIGH_RESOLUTION_NET.STAGE3.NUM_CHANNELS = [48, 96, 192]\n",
    "HIGH_RESOLUTION_NET.STAGE3.BLOCK = 'BASIC'\n",
    "HIGH_RESOLUTION_NET.STAGE3.FUSE_METHOD = 'SUM'\n",
    "\n",
    "HIGH_RESOLUTION_NET.STAGE4 = CN()\n",
    "HIGH_RESOLUTION_NET.STAGE4.NUM_MODULES = 3\n",
    "HIGH_RESOLUTION_NET.STAGE4.NUM_BRANCHES = 4\n",
    "HIGH_RESOLUTION_NET.STAGE4.NUM_BLOCKS = [4, 4, 4, 4]\n",
    "HIGH_RESOLUTION_NET.STAGE4.NUM_CHANNELS = [48, 96, 192, 384]\n",
    "HIGH_RESOLUTION_NET.STAGE4.BLOCK = 'BASIC'\n",
    "HIGH_RESOLUTION_NET.STAGE4.FUSE_METHOD = 'SUM'\n",
    "\n",
    "DATASET = CN()\n",
    "DATASET.ROOT = ''\n",
    "DATASET.DATASET = 'cityscapes'\n",
    "DATASET.NUM_CLASSES = 19\n",
    "DATASET.TRAIN_SET = 'list/cityscapes/train.lst'\n",
    "DATASET.EXTRA_TRAIN_SET = ''\n",
    "DATASET.TEST_SET = 'list/cityscapes/val.lst'\n",
    "\n",
    "MODEL = CN()\n",
    "MODEL.NAME = 'seg_hrnet'\n",
    "MODEL.PRETRAINED = '/home/sourajit/DeepLearningProjects/Research_2.0/hrnet_cs_8090_torch11.pth'\n",
    "MODEL.ALIGN_CORNERS = True\n",
    "MODEL.NUM_OUTPUTS = 1\n",
    "MODEL.EXTRA = CN(new_allowed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighResolutionNetv1(nn.Module):\n",
    "\n",
    "    def __init__(self, model_config=None, dataset_config=None):\n",
    "        model_config = model_config\n",
    "        dataset_config = dataset_config\n",
    "        super(HighResolutionNetv1, self).__init__()\n",
    "\n",
    "        # stem net\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.stage1_cfg = model_config['STAGE1']\n",
    "        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n",
    "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
    "        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n",
    "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
    "        stage1_out_channel = block.expansion*num_channels\n",
    "\n",
    "        self.stage2_cfg = model_config['STAGE2']\n",
    "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition1 = self._make_transition_layer(\n",
    "            [stage1_out_channel], num_channels)\n",
    "        self.stage2, pre_stage_channels = self._make_stage(\n",
    "            self.stage2_cfg, num_channels)\n",
    "\n",
    "        self.stage3_cfg = model_config['STAGE3']\n",
    "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition2 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage3, pre_stage_channels = self._make_stage(\n",
    "            self.stage3_cfg, num_channels)\n",
    "\n",
    "        self.stage4_cfg = model_config['STAGE4']\n",
    "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition3 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage4, pre_stage_channels = self._make_stage(\n",
    "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
    "        \n",
    "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
    "        self.before_last_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=last_inp_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0),\n",
    "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=last_inp_channels,\n",
    "                kernel_size=model_config.FINAL_CONV_KERNEL,\n",
    "                stride=1,\n",
    "                padding=1 if model_config.FINAL_CONV_KERNEL == 3 else 0)\n",
    "        )\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=last_inp_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0),\n",
    "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=dataset_config.NUM_CLASSES,\n",
    "                kernel_size=model_config.FINAL_CONV_KERNEL,\n",
    "                stride=1,\n",
    "                padding=1 if model_config.FINAL_CONV_KERNEL == 3 else 0)\n",
    "        )\n",
    "\n",
    "    def _make_transition_layer(\n",
    "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "\n",
    "        transition_layers = []\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    transition_layers.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_channels_pre_layer[i],\n",
    "                                  num_channels_cur_layer[i],\n",
    "                                  3,\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  bias=False),\n",
    "                        BatchNorm2d(\n",
    "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=True)))\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                conv3x3s = []\n",
    "                for j in range(i+1-num_branches_pre):\n",
    "                    inchannels = num_channels_pre_layer[-1]\n",
    "                    outchannels = num_channels_cur_layer[i] \\\n",
    "                        if j == i-num_branches_pre else inchannels\n",
    "                    conv3x3s.append(nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
    "                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=True)))\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
    "\n",
    "        return nn.ModuleList(transition_layers)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride, downsample))\n",
    "        inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_stage(self, layer_config, num_inchannels,\n",
    "                    multi_scale_output=True):\n",
    "        num_modules = layer_config['NUM_MODULES']\n",
    "        num_branches = layer_config['NUM_BRANCHES']\n",
    "        num_blocks = layer_config['NUM_BLOCKS']\n",
    "        num_channels = layer_config['NUM_CHANNELS']\n",
    "        block = blocks_dict[layer_config['BLOCK']]\n",
    "        fuse_method = layer_config['FUSE_METHOD']\n",
    "\n",
    "        modules = []\n",
    "        for i in range(num_modules):\n",
    "            # multi_scale_output is only used last module\n",
    "            if not multi_scale_output and i == num_modules - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "            modules.append(\n",
    "                HighResolutionModule(num_branches,\n",
    "                                      block,\n",
    "                                      num_blocks,\n",
    "                                      num_inchannels,\n",
    "                                      num_channels,\n",
    "                                      fuse_method,\n",
    "                                      reset_multi_scale_output)\n",
    "            )\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        return nn.Sequential(*modules), num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_h, out_w = x.size(2), x.size(3)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print (\"After first few: \", x.shape)\n",
    "        x = self.layer1(x)\n",
    "        #print (\"After stage 1: \", x.shape)\n",
    "        x_list = []\n",
    "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
    "            if self.transition1[i] is not None:\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list = self.stage2(x_list)\n",
    "        #for idx in range(len(y_list)):\n",
    "            #print (\"After stage 2: \", y_list[idx].shape)\n",
    "        x_list = []\n",
    "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
    "            if self.transition2[i] is not None:\n",
    "                x_list.append(self.transition2[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage3(x_list)\n",
    "        #for idx in range(len(y_list)):\n",
    "            #print (\"After stage 3: \", y_list[idx].shape)\n",
    "        x_list = []\n",
    "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
    "            if self.transition3[i] is not None:\n",
    "                x_list.append(self.transition3[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        x = self.stage4(x_list)\n",
    "        #for idx in range(len(x)):\n",
    "            #print (\"After stage 4: \", x[idx].shape)\n",
    "        # Upsampling\n",
    "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
    "        #print (\"..........\", x0_h, x0_w)\n",
    "        x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear',\n",
    "                        align_corners=ALIGN_CORNERS)\n",
    "        #print (\"After upsample1: \", x1.shape)\n",
    "        x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear',\n",
    "                        align_corners=ALIGN_CORNERS)\n",
    "        #print (\"After upsample2: \", x2.shape)\n",
    "        x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear',\n",
    "                        align_corners=ALIGN_CORNERS)\n",
    "        #print (\"After upsample3: \", x3.shape)\n",
    "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
    "        #print (\"After cat: \", x.shape)\n",
    "        x = self.before_last_layer(x)\n",
    "        #print (\"After last layer 1: \", x.shape)\n",
    "        x = F.interpolate(x, size=(2*x.size(2), 2*x.size(3)), mode='bilinear', align_corners=ALIGN_CORNERS)\n",
    "        x = self.before_last_layer(x)\n",
    "        #print (\"After last layer 2: \", x.shape)\n",
    "        x = F.interpolate(x, size=(2*x.size(2), 2*x.size(3)), mode='bilinear', align_corners=ALIGN_CORNERS)\n",
    "        x = self.last_layer(x)\n",
    "        #print (\"After the very last layer: \", x.shape)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self, pretrained='',):\n",
    "        logger.info('=> init weights from normal distribution')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.001)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        if os.path.isfile(pretrained):\n",
    "            pretrained_dict = torch.load(pretrained)\n",
    "            #print (len(pretrained_dict))\n",
    "            logger.info('=> loading pretrained model {}'.format(pretrained))\n",
    "            model_dict = self.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                               if k in model_dict.keys()}\n",
    "            #for k, _ in pretrained_dict.items():\n",
    "            #    logger.info(\n",
    "            #        '=> loading {} pretrained model {}'.format(k, pretrained))\n",
    "            model_dict.update(pretrained_dict)\n",
    "            self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighResolutionNet(nn.Module):\n",
    "\n",
    "    def __init__(self, model_config=None, dataset_config=None):\n",
    "        model_config = model_config\n",
    "        dataset_config = dataset_config\n",
    "        super(HighResolutionNet, self).__init__()\n",
    "\n",
    "        # stem net\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.stage1_cfg = model_config['STAGE1']\n",
    "        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n",
    "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
    "        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n",
    "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
    "        stage1_out_channel = block.expansion*num_channels\n",
    "\n",
    "        self.stage2_cfg = model_config['STAGE2']\n",
    "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition1 = self._make_transition_layer(\n",
    "            [stage1_out_channel], num_channels)\n",
    "        self.stage2, pre_stage_channels = self._make_stage(\n",
    "            self.stage2_cfg, num_channels)\n",
    "\n",
    "        self.stage3_cfg = model_config['STAGE3']\n",
    "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition2 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage3, pre_stage_channels = self._make_stage(\n",
    "            self.stage3_cfg, num_channels)\n",
    "\n",
    "        self.stage4_cfg = model_config['STAGE4']\n",
    "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition3 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage4, pre_stage_channels = self._make_stage(\n",
    "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
    "        \n",
    "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=last_inp_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0),\n",
    "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=dataset_config.NUM_CLASSES,\n",
    "                kernel_size=model_config.FINAL_CONV_KERNEL,\n",
    "                stride=1,\n",
    "                padding=1 if model_config.FINAL_CONV_KERNEL == 3 else 0)\n",
    "        )\n",
    "\n",
    "    def _make_transition_layer(\n",
    "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "\n",
    "        transition_layers = []\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    transition_layers.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_channels_pre_layer[i],\n",
    "                                  num_channels_cur_layer[i],\n",
    "                                  3,\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  bias=False),\n",
    "                        BatchNorm2d(\n",
    "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=True)))\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                conv3x3s = []\n",
    "                for j in range(i+1-num_branches_pre):\n",
    "                    inchannels = num_channels_pre_layer[-1]\n",
    "                    outchannels = num_channels_cur_layer[i] \\\n",
    "                        if j == i-num_branches_pre else inchannels\n",
    "                    conv3x3s.append(nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
    "                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=True)))\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
    "\n",
    "        return nn.ModuleList(transition_layers)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride, downsample))\n",
    "        inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_stage(self, layer_config, num_inchannels,\n",
    "                    multi_scale_output=True):\n",
    "        num_modules = layer_config['NUM_MODULES']\n",
    "        num_branches = layer_config['NUM_BRANCHES']\n",
    "        num_blocks = layer_config['NUM_BLOCKS']\n",
    "        num_channels = layer_config['NUM_CHANNELS']\n",
    "        block = blocks_dict[layer_config['BLOCK']]\n",
    "        fuse_method = layer_config['FUSE_METHOD']\n",
    "\n",
    "        modules = []\n",
    "        for i in range(num_modules):\n",
    "            # multi_scale_output is only used last module\n",
    "            if not multi_scale_output and i == num_modules - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "            modules.append(\n",
    "                HighResolutionModule(num_branches,\n",
    "                                      block,\n",
    "                                      num_blocks,\n",
    "                                      num_inchannels,\n",
    "                                      num_channels,\n",
    "                                      fuse_method,\n",
    "                                      reset_multi_scale_output)\n",
    "            )\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        return nn.Sequential(*modules), num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_h, out_w = x.size(2), x.size(3)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        #print (\"After first few: \", x.shape)\n",
    "        x = self.layer1(x)\n",
    "        #print (\"After stage 1: \", x.shape)\n",
    "        x_list = []\n",
    "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
    "            if self.transition1[i] is not None:\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list = self.stage2(x_list)\n",
    "        #for idx in range(len(y_list)):\n",
    "            #print (\"After stage 2: \", y_list[idx].shape)\n",
    "        x_list = []\n",
    "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
    "            if self.transition2[i] is not None:\n",
    "                x_list.append(self.transition2[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage3(x_list)\n",
    "        #for idx in range(len(y_list)):\n",
    "            #print (\"After stage 3: \", y_list[idx].shape)\n",
    "        x_list = []\n",
    "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
    "            if self.transition3[i] is not None:\n",
    "                x_list.append(self.transition3[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        x = self.stage4(x_list)\n",
    "        #for idx in range(len(x)):\n",
    "            #print (\"After stage 4: \", x[idx].shape)\n",
    "        # Upsampling\n",
    "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
    "        #print (\"..........\", x0_h, x0_w)\n",
    "        x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear',\n",
    "                        align_corners=ALIGN_CORNERS)\n",
    "        #print (\"After upsample1: \", x1.shape)\n",
    "        x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear',\n",
    "                        align_corners=ALIGN_CORNERS)\n",
    "        #print (\"After upsample2: \", x2.shape)\n",
    "        x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear',\n",
    "                        align_corners=ALIGN_CORNERS)\n",
    "        #print (\"After upsample3: \", x3.shape)\n",
    "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
    "        #print (\"After cat: \", x.shape)\n",
    "        x = self.last_layer(x)\n",
    "        #print (\"After the very last layer: \", x.shape)\n",
    "        #x = F.interpolate(x, size=(out_h, out_w), mode='bilinear', align_corners=ALIGN_CORNERS)\n",
    "        #print (\"After final upsampling: \", x.shape)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self, pretrained='',):\n",
    "        logger.info('=> init weights from normal distribution')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.001)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        if os.path.isfile(pretrained):\n",
    "            pretrained_dict = torch.load(pretrained)\n",
    "            #print (len(pretrained_dict))\n",
    "            logger.info('=> loading pretrained model {}'.format(pretrained))\n",
    "            model_dict = self.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                               if k in model_dict.keys()}\n",
    "            #for k, _ in pretrained_dict.items():\n",
    "            #    logger.info(\n",
    "            #        '=> loading {} pretrained model {}'.format(k, pretrained))\n",
    "            model_dict.update(pretrained_dict)\n",
    "            self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_num_param(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param(num_param):\n",
    "    num_param = str(num_param)\n",
    "    ret = []\n",
    "    length = len(num_param)\n",
    "    for i in range(length):\n",
    "        idx = i+1\n",
    "        if (idx>1 and (idx-1)%3==0):\n",
    "            ret.append(',')\n",
    "        ret.append(num_param[length-idx])\n",
    "    ret.reverse()\n",
    "    ret = ''.join(ret)\n",
    "    temp_a = '## Number of parameters: ' + ret  + ' ##'\n",
    "    temp_b = '#' * len(temp_a)\n",
    "    columns = shutil.get_terminal_size().columns\n",
    "    print (f'{temp_b}'.center(columns))\n",
    "    print (f'{temp_a}'.center(columns))\n",
    "    print (f'{temp_b}'.center(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_seg_model(HIGH_RESOLUTION_NET, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
