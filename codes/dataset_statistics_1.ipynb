{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from yacs.config import CfgNode as CN\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "#from cityscapesscripts.helpers.labels import trainId2label as t2l\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "import shutil\n",
    "import time\n",
    "import tqdm\n",
    "from csv import reader\n",
    "from functools import reduce\n",
    "import torchvision\n",
    "import torch._utils\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import math\n",
    "from torchvision import models\n",
    "import progressbar\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "import gc\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import logging\n",
    "import functools\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/home/sourajit/DeepLearningProjects/data/cityscapes'\n",
    "IMG_HEIGHT = 512  \n",
    "IMG_WIDTH = 1024\n",
    "ORIGINAL_IMG_HEIGHT = 1024  \n",
    "ORIGINAL_IMG_WIDTH = 2048\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 10\n",
    "GLOBAL_EPOCH_COUNTER = 0\n",
    "MAX_ITER = 3000000\n",
    "NUM_WORKERS = 0\n",
    "NUM_CLASSES = 19\n",
    "ignore_label = 255\n",
    "BatchNorm2d = nn.BatchNorm2d\n",
    "BN_MOMENTUM = 0.01\n",
    "ALIGN_CORNERS = True\n",
    "logger = logging.getLogger(__name__)\n",
    "device_rtx3090 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device_rtx3070 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(filename):\n",
    "    filename = str(filename)\n",
    "    data_type = None\n",
    "    if (filename == 'None'):\n",
    "        data_type = 'Train Set'\n",
    "    elif (filename.startswith(\"['/frankfurt\") or\n",
    "         filename.startswith(\"['/lindau\") or\n",
    "         filename.startswith(\"['/munster\")):\n",
    "        data_type = 'Val Set'\n",
    "    else:\n",
    "        data_type = 'Test Set'\n",
    "    return data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_performance():\n",
    "    TRAIN_LOSS_CSV = pd.read_csv(TRAIN_LOSS_DIR)\n",
    "    TRAIN_LR_CSV = pd.read_csv(TRAIN_LR_DIR)\n",
    "    TRAIN_meanIOU_CSV = pd.read_csv(TRAIN_meanIOU_DIR)\n",
    "    TRAIN_MODEL_UPDATE_CSV = pd.read_csv(TRAIN_MODEL_UPDATE_DIR)\n",
    "    vis_train_loss_iter = []\n",
    "    vis_train_loss_epoch_temp = []\n",
    "    vis_train_loss_epoch = []\n",
    "    vis_train_lr_iter = []\n",
    "    vis_train_miou_epoch = []\n",
    "    vis_train_model_update_epoch = []\n",
    "    num_epochs = len(TRAIN_meanIOU_CSV)+1\n",
    "    num_iterations = len(TRAIN_LOSS_CSV)+1\n",
    "    num_batches = int(num_iterations/num_epochs)\n",
    "    cnt = 0\n",
    "    temp_sum = 0\n",
    "    with open(TRAIN_LOSS_DIR, 'r')as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        for current_loss in csv_reader:\n",
    "            cnt += 1\n",
    "            vis_train_loss_iter.append(float(current_loss[0]))\n",
    "            temp_sum += float(current_loss[0])\n",
    "            if (cnt % num_batches == 0):\n",
    "                vis_train_loss_epoch_temp.append(temp_sum/num_batches)\n",
    "                temp_sum = 0\n",
    "    for epc in range(num_epochs):\n",
    "        for btch in range(num_batches):\n",
    "            vis_train_loss_epoch.append(vis_train_loss_epoch_temp[epc])\n",
    "    with open(TRAIN_LR_DIR, 'r')as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        for current_lr in csv_reader:\n",
    "            vis_train_lr_iter.append(float(current_lr[0]))\n",
    "    with open(TRAIN_meanIOU_DIR, 'r')as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        for current_miou in csv_reader:\n",
    "            for btch in range(num_batches):\n",
    "                vis_train_miou_epoch.append(float(current_miou[0])/100.0)\n",
    "    with open(TRAIN_MODEL_UPDATE_DIR, 'r')as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        for current_model_update in csv_reader:\n",
    "            for btch in range(num_batches):\n",
    "                vis_train_model_update_epoch.append(float(current_model_update[0]))\n",
    "    fig, axs = plt.subplots(1, figsize=(20, 12))\n",
    "    axs.plot([x for x in range(num_iterations)], vis_train_loss_iter)\n",
    "    axs.plot([x for x in range(num_iterations)], vis_train_loss_epoch)\n",
    "    axs.plot([x for x in range(num_iterations)], vis_train_lr_iter)\n",
    "    axs.plot([x for x in range(num_iterations)], vis_train_miou_epoch)\n",
    "    axs.plot([x for x in range(num_iterations)], vis_train_model_update_epoch)\n",
    "    axs.set_xlabel('Training Iterations')\n",
    "    axs.set_ylabel('Training Loss, Learning Rate, mIOU, Model Update Status')\n",
    "    axs.legend(['Iteration Loss', 'Epoch Loss', 'Epoch Learning Rate', 'Epoch mIOU', \n",
    "                'Model Update'],loc='best')\n",
    "    fig.suptitle('Outlining Training Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_images(tensor_pred, folder, image_name):\n",
    "    tensor_pred = transforms.ToPILImage()(255*tensor_pred[0].type(torch.FloatTensor))\n",
    "    filename = f\"{folder}/{image_name}.png\"\n",
    "    tensor_pred.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return reduce(lambda a, b: a + b, lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapping_before_conversion = {\n",
    "     0 : (  0,  0,  0),\n",
    "     1 : (  0,  0,  0),\n",
    "     2 : (  0,  0,  0),\n",
    "     3 : (  0,  0,  0),\n",
    "     4 : (  0,  0,  0),\n",
    "     5 : (111, 74,  0),\n",
    "     6 : ( 81,  0, 81),\n",
    "     7 : (128, 64,128),\n",
    "     8 : (244, 35,232),\n",
    "     9 : (250,170,160),\n",
    "    10 : (230,150,140),\n",
    "    11 : ( 70, 70, 70),\n",
    "    12 : (102,102,156),\n",
    "    13 : (190,153,153),\n",
    "    14 : (180,165,180),\n",
    "    15 : (150,100,100),\n",
    "    16 : (150,120, 90),\n",
    "    17 : (153,153,153),\n",
    "    18 : (153,153,153),\n",
    "    19 : (250,170, 30),\n",
    "    20 : (220,220,  0),\n",
    "    21 : (107,142, 35),\n",
    "    22 : (152,251,152),\n",
    "    23 : ( 70,130,180),\n",
    "    24 : (220, 20, 60),\n",
    "    25 : (255,  0,  0),\n",
    "    26 : (  0,  0,142),\n",
    "    27 : (  0,  0, 70),\n",
    "    28 : (  0, 60,100),\n",
    "    29 : (  0,  0, 90),\n",
    "    30 : (  0,  0,110),\n",
    "    31 : (  0, 80,100),\n",
    "    32 : (  0,  0,230),\n",
    "    33 : (119, 11, 32),\n",
    "    -1 : (  0,  0,142)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapping_after_conversion = {\n",
    "     255 : (  0,  0,  0),\n",
    "     0 : (128, 64,128),\n",
    "     1 : (244, 35,232),\n",
    "     2 : ( 70, 70, 70),\n",
    "     3 : (102,102,156),\n",
    "     4 : (190,153,153),\n",
    "     5 : (153,153,153),\n",
    "     6 : (250,170, 30),\n",
    "     7 : (220,220,  0),\n",
    "     8 : (107,142, 35),\n",
    "     9 : (152,251,152),\n",
    "    10 : ( 70,130,180),\n",
    "    11 : (220, 20, 60),\n",
    "    12 : (255,  0,  0),\n",
    "    13 : (  0,  0,142),\n",
    "    14 : (  0,  0, 70),\n",
    "    15 : (  0, 60,100),\n",
    "    16 : (  0, 80,100),\n",
    "    17 : (  0,  0,230),\n",
    "    18 : (119, 11, 32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2class_after_conversion = {\n",
    "     255 : \"unlabeled\",\n",
    "     0 : \"road\",\n",
    "     1 : \"sidewalk\",\n",
    "     2 : \"building\",\n",
    "     3 : \"wall\",\n",
    "     4 : \"fence\",\n",
    "     5 : \"pole\",\n",
    "     6 : \"traffic light\",\n",
    "     7 : \"traffic sign\",\n",
    "     8 : \"vegetation\",\n",
    "     9 : \"terrain\",\n",
    "    10 : \"sky\",\n",
    "    11 : \"preson\",\n",
    "    12 : \"rider\",\n",
    "    13 : \"car\",\n",
    "    14 : \"truck\",\n",
    "    15 : \"bus\",\n",
    "    16 : \"train\",\n",
    "    17 : \"motorcycle\",\n",
    "    18 : \"bicycle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2color(label, color_array):\n",
    "    temp = label.clone()\n",
    "    label1 = label.clone()\n",
    "    label2 = label.clone()\n",
    "    label3 = label.clone()\n",
    "    #print (label.dtype, temp.dtype, label1.dtype, label2.dtype, label3.dtype)\n",
    "    for k, v in color_array.items():\n",
    "        label1[temp == k] = v[0]\n",
    "        label2[temp == k] = v[1]\n",
    "        label3[temp == k] = v[2]\n",
    "    #print (label1.shape, label2.shape, label3.shape)\n",
    "    label1 = rearrange(label1, 'd0 d1 d2 -> d0 1 d1 d2')\n",
    "    label2 = rearrange(label2, 'd0 d1 d2 -> d0 1 d1 d2')\n",
    "    label3 = rearrange(label3, 'd0 d1 d2 -> d0 1 d1 d2')\n",
    "    #print (label1.shape, label2.shape, label3.shape)\n",
    "    ret = torch.stack((label1, label2, label3), 1)\n",
    "    ret = rearrange(ret, 'd0 d1 d2 d3 d4-> d0 (d1 d2) d3 d4')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {-1: 255, 0: 255, \n",
    "                  1: 255, 2: 255, \n",
    "                  3: 255, 4: 255, \n",
    "                  5: 255, 6: 255, \n",
    "                  7: 0, 8: 1, 9: 255, \n",
    "                  10: 255, 11: 2, 12: 3, \n",
    "                  13: 4, 14: 255, 15: 255, \n",
    "                  16: 255, 17: 5, 18: 255, \n",
    "                  19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11,\n",
    "                  25: 12, 26: 13, 27: 14, 28: 15, \n",
    "                  29: 255, 30: 255, \n",
    "                  31: 16, 32: 17, 33: 18}\n",
    "class_weights = torch.tensor([0.8373, 0.918, 0.866, 1.0345, \n",
    "                              1.0166, 0.9969, 0.9754, 1.0489,\n",
    "                              0.8786, 1.0023, 0.9539, 0.9843, \n",
    "                              1.1116, 0.9037, 1.0865, 1.0955, \n",
    "                              1.0865, 1.1529, 1.0507],\n",
    "                             dtype=torch.float32,\n",
    "                             device=device_rtx3090)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_label(label):\n",
    "    label[label < -1] = 0\n",
    "    label[label > 33] = 0\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label, inverse=False):\n",
    "    #temp = label.clone()\n",
    "    if inverse:\n",
    "        for v, k in label_mapping.items():\n",
    "            label[label == k] = v\n",
    "    else:\n",
    "        for k, v in label_mapping.items():\n",
    "            label[label == k] = v\n",
    "        label[label < 0] = 255\n",
    "        label[label > 18] = 255\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_label_after_loader(label):\n",
    "    label[label < 0] = 255\n",
    "    label[label > 18] = 255\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, split, root_dir, relabelled=True, target_type='semantic', mode='fine', transform=None, eval=False):\n",
    "        self.transform = transform\n",
    "        if mode == 'fine':\n",
    "            self.mode = 'gtFine'\n",
    "        elif mode == 'coarse':\n",
    "            self.mode = 'gtCoarse'\n",
    "        self.split = split\n",
    "        self.yLabel_list = []\n",
    "        self.XImg_list = []\n",
    "        self.eval = eval \n",
    "        self.label_path = os.path.join(os.getcwd(), root_dir+'/'+self.mode+'/'+self.split)\n",
    "        self.rgb_path = os.path.join(os.getcwd(), root_dir+'/leftImg8bit/'+self.split)\n",
    "        city_list = os.listdir(self.label_path)\n",
    "        for city in city_list:\n",
    "            self.XImg_list.extend(\n",
    "                ['/'+city+'/'+path for path in os.listdir(self.rgb_path+'/'+city)]\n",
    "            )\n",
    "        for i in range(len(self.XImg_list)):\n",
    "            self.yLabel_list.append(self.XImg_list[i][:-15]+\"gtFine_labelIds.png\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        length = len(self.XImg_list)\n",
    "        return length\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.rgb_path+self.XImg_list[index])\n",
    "        y = Image.open(self.label_path+self.yLabel_list[index])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            y = self.transform(y)\n",
    "\n",
    "        image = transforms.ToTensor()(image)\n",
    "        y = np.array(y)\n",
    "        y = torch.from_numpy(y)\n",
    "        \n",
    "        y = y.type(torch.LongTensor)\n",
    "        y = refine_label(y)\n",
    "        y_converted = convert_label(y)\n",
    "        if self.eval:\n",
    "            return image, y_converted, self.XImg_list[index]\n",
    "        else:\n",
    "            return image, y_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cityscapes_data(\n",
    "    mode,\n",
    "    split,\n",
    "    relabelled=True,\n",
    "    root_dir=ROOT_DIR,\n",
    "    target_type=\"semantic\",\n",
    "    transforms=None,\n",
    "    batch_size=1,\n",
    "    eval=False,\n",
    "    shuffle=False,\n",
    "    pin_memory=True):\n",
    "    data = CityscapesDataset(\n",
    "        mode=mode, split=split, target_type=target_type, relabelled=relabelled, transform=transforms, root_dir=root_dir, eval=eval)\n",
    "\n",
    "    data_loaded = torch.utils.data.DataLoader(\n",
    "        data, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory, num_workers=NUM_WORKERS)\n",
    "\n",
    "    return data_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_cityscapes_data(\n",
    "    split='train',\n",
    "    mode='fine',\n",
    "    relabelled=True,\n",
    "    root_dir=ROOT_DIR,\n",
    "    transforms=transform,\n",
    "    batch_size=BATCH_SIZE*0+1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = get_cityscapes_data(\n",
    "    split='val',\n",
    "    mode='fine',\n",
    "    relabelled=True,\n",
    "    root_dir=ROOT_DIR,\n",
    "    shuffle=False,\n",
    "    transforms=None,\n",
    "    batch_size=1,\n",
    "    eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = get_cityscapes_data(\n",
    "    split='test',\n",
    "    mode='fine',\n",
    "    relabelled=True,\n",
    "    root_dir=ROOT_DIR,\n",
    "    shuffle=False,\n",
    "    transforms=None,\n",
    "    batch_size=1,\n",
    "    eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint (len(train_set))\\nexamples = next(iter(train_set))\\ninputs, labels = examples\\nprint (inputs.shape, inputs.dtype, labels.shape, labels.dtype)\\ncolor_labels = label2color(labels, color_mapping_after_conversion)\\n\\nfig,ax=plt.subplots(ncols=2,nrows=BATCH_SIZE*0+1,figsize=(24,5*(BATCH_SIZE*0+1)))\\nfor current_batch in range(BATCH_SIZE*0+1):    \\n    #ax[current_batch][0].imshow(transforms.ToPILImage()(inputs[current_batch].type(torch.FloatTensor)))\\n    #ax[current_batch][1].imshow(transforms.ToPILImage()(255*color_labels[current_batch].type(torch.FloatTensor)))\\n    ax[0].imshow(transforms.ToPILImage()(inputs[current_batch].type(torch.FloatTensor)))\\n    ax[1].imshow(transforms.ToPILImage()(255*color_labels[current_batch].type(torch.FloatTensor)))\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print (len(train_set))\n",
    "examples = next(iter(train_set))\n",
    "inputs, labels = examples\n",
    "print (inputs.shape, inputs.dtype, labels.shape, labels.dtype)\n",
    "color_labels = label2color(labels, color_mapping_after_conversion)\n",
    "\n",
    "fig,ax=plt.subplots(ncols=2,nrows=BATCH_SIZE*0+1,figsize=(24,5*(BATCH_SIZE*0+1)))\n",
    "for current_batch in range(BATCH_SIZE*0+1):    \n",
    "    #ax[current_batch][0].imshow(transforms.ToPILImage()(inputs[current_batch].type(torch.FloatTensor)))\n",
    "    #ax[current_batch][1].imshow(transforms.ToPILImage()(255*color_labels[current_batch].type(torch.FloatTensor)))\n",
    "    ax[0].imshow(transforms.ToPILImage()(inputs[current_batch].type(torch.FloatTensor)))\n",
    "    ax[1].imshow(transforms.ToPILImage()(255*color_labels[current_batch].type(torch.FloatTensor)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2975/2975 [04:23<00:00, 11.29it/s]\n"
     ]
    }
   ],
   "source": [
    "data = train_set\n",
    "PIXEL_COUNT = [0] * (NUM_CLASSES+1)\n",
    "for batch_no, example in enumerate(tqdm.tqdm(data)):\n",
    "    X, y, filename = None, None, None\n",
    "    if (len(example) == 2):\n",
    "        X, y = example\n",
    "    elif  (len(example) == 3):\n",
    "        X, y, filename = example\n",
    "    y = refine_label_after_loader(y)\n",
    "    \n",
    "    #print (X.shape, X.dtype, y.shape, y.dtype, torch.min(y), torch.max(y))\n",
    "    uniques = torch.unique(y, return_counts=True)\n",
    "    \n",
    "    pixel_cnt =  [0] * (NUM_CLASSES+1)\n",
    "    # Pixel Count for Foreground (labeled) objects\n",
    "    for i in range(uniques[0].shape[0]):\n",
    "        current_class = uniques[0][i].item()\n",
    "        pixels_in_current_class = uniques[1][i].item()\n",
    "        if (0 <= current_class and current_class < NUM_CLASSES):\n",
    "            pixel_cnt[current_class] += pixels_in_current_class\n",
    "    # Pixel Count for Background (unlabeled) objects\n",
    "    pixel_cnt[NUM_CLASSES] = ( X.shape[-2] * X.shape[-1] ) - \\\n",
    "                             ( sum(pixel_cnt[0:NUM_CLASSES]) )\n",
    "    \n",
    "    #print (pixel_cnt)\n",
    "    PIXEL_COUNT = [a + b for a, b in zip(PIXEL_COUNT, pixel_cnt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[506561734,\n",
       " 83702236,\n",
       " 306348833,\n",
       " 13281836,\n",
       " 14202183,\n",
       " 13846171,\n",
       " 5029042,\n",
       " 9728149,\n",
       " 214915285,\n",
       " 17140763,\n",
       " 54718788,\n",
       " 16537777,\n",
       " 2408445,\n",
       " 93896554,\n",
       " 3705558,\n",
       " 3235862,\n",
       " 3230374,\n",
       " 1378319,\n",
       " 5162611,\n",
       " 190726280]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIXEL_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pixels = sum(PIXEL_COUNT)\n",
    "pixelwise_class_distribution = [100*elem/total_pixels for elem in PIXEL_COUNT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([32.476969101849726,\n",
       "  5.3663645511915705,\n",
       "  19.640807656680835,\n",
       "  0.8515324953223476,\n",
       "  0.9105382967395943,\n",
       "  0.8877134563542214,\n",
       "  0.3224247523716518,\n",
       "  0.6236965275612198,\n",
       "  13.778768908075925,\n",
       "  1.0989381806189273,\n",
       "  3.508161528771665,\n",
       "  1.0602792050658154,\n",
       "  0.1544115723682051,\n",
       "  6.019948366309414,\n",
       "  0.23757280622209823,\n",
       "  0.20745939366957722,\n",
       "  0.2071075439453125,\n",
       "  0.08836755832704175,\n",
       "  0.3309882027762277,\n",
       "  12.227949895778623],\n",
       " 99.99999999999999)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixelwise_class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
